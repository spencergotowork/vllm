# ğŸš€ vLLM æºç æ·±åº¦å‰–æä¸æ¶æ„è¿›é˜¶

> æœ¬æ–‡æ¡£é¢å‘æœ‰ä¸€å¹´ä»¥ä¸Š AI Infra ç»éªŒçš„å·¥ç¨‹å¸ˆï¼Œæ·±å…¥è§£æ vLLM çš„ä»£ç å®ç°æµç¨‹ã€å†…å­˜ç‰©ç†å¸ƒå±€ã€åˆ†å¸ƒå¼è®¾è®¡ä»¥åŠå¤šæ¨¡æ€é€‚é…æœºåˆ¶ã€‚

---

## ç›®å½•

- [Phase 1: æ ¸å¿ƒè°ƒåº¦ä¸å†…å­˜åˆ†é… (The Brain)](#phase-1-æ ¸å¿ƒè°ƒåº¦ä¸å†…å­˜åˆ†é…-the-brain)
- [Phase 2: ç‰©ç†å†…å­˜ç®¡ç†ä¸ Worker æ‰§è¡Œ (The Body)](#phase-2-ç‰©ç†å†…å­˜ç®¡ç†ä¸-worker-æ‰§è¡Œ-the-body)
- [Phase 3: Kernel å®ç°ä¸ PagedAttention (The Heart)](#phase-3-kernel-å®ç°ä¸-pagedattention-the-heart)
- [Phase 4: å¤šæ¨¡æ€æ‰©å±• (The Eyes)](#phase-4-å¤šæ¨¡æ€æ‰©å±•-the-eyes)
- [å®Œæ•´è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ](#å®Œæ•´è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ)
- [åˆ†å¸ƒå¼æ¶æ„è®¾è®¡](#åˆ†å¸ƒå¼æ¶æ„è®¾è®¡)
- [å®è·µæŒ‡å—ä¸åŠ¨æ‰‹ç»ƒä¹ ](#å®è·µæŒ‡å—ä¸åŠ¨æ‰‹ç»ƒä¹ )

---

## Phase 1: æ ¸å¿ƒè°ƒåº¦ä¸å†…å­˜åˆ†é… (The Brain)

### æ ¸å¿ƒé€»è¾‘è§£æ

vLLM çš„è°ƒåº¦ç³»ç»Ÿé‡‡ç”¨äº†ç±»ä¼¼ **OS è™šæ‹Ÿå†…å­˜ç®¡ç†** çš„è®¾è®¡å“²å­¦ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

1. **Scheduler ä¸åšå®é™…æ¨ç†**ï¼Œåªè´Ÿè´£"èµ„æºè°ƒåº¦å†³ç­–"
2. **é€»è¾‘å—ï¼ˆLogical Blockï¼‰** å¯¹åº”è¯·æ±‚çš„ token åºåˆ—ï¼Œæ˜¯è°ƒåº¦å™¨è§†è§’çš„æŠ½è±¡
3. **ç‰©ç†å—ï¼ˆPhysical Blockï¼‰** å¯¹åº” GPU æ˜¾å­˜ä¸­çš„å®é™…å­˜å‚¨ä½ç½®

**è°ƒåº¦å¾ªç¯çš„æ•°æ®æµï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler.schedule()                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ä¼˜å…ˆè°ƒåº¦ RUNNING é˜Ÿåˆ—ä¸­çš„è¯·æ±‚                         â”‚
â”‚     â†“                                                    â”‚
â”‚  2. kv_cache_manager.allocate_slots() â†’ åˆ†é… KV å—       â”‚
â”‚     â†“                                                    â”‚
â”‚  3. å¦‚æœåˆ†é…å¤±è´¥ â†’ æ‰§è¡Œ Preemption (æŠ¢å ä½ä¼˜å…ˆçº§è¯·æ±‚)      â”‚
â”‚     â†“                                                    â”‚
â”‚  4. è°ƒåº¦ WAITING é˜Ÿåˆ—ä¸­çš„æ–°è¯·æ±‚                           â”‚
â”‚     â†“                                                    â”‚
â”‚  5. è¿”å› SchedulerOutput (åŒ…å« block_ids, num_tokens ç­‰)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ä»£ç å®šä½

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | æ ¸å¿ƒç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------------|------|
| **è°ƒåº¦å™¨ä¸»é€»è¾‘** | `vllm/v1/core/sched/scheduler.py` | `Scheduler.schedule()` (L191-L715) | æ¯ä¸ªè°ƒåº¦æ­¥çš„æ ¸å¿ƒå†³ç­– |
| **KV ç¼“å­˜ç®¡ç†å™¨** | `vllm/v1/core/kv_cache_manager.py` | `KVCacheManager.allocate_slots()` (L217-L332) | åˆ†é…/é‡Šæ”¾é€»è¾‘å— |
| **å—æ± ç®¡ç†** | `vllm/v1/core/kv_cache_utils.py` | `KVCacheBlock` (L107-L152) | ç‰©ç†å—çš„å…ƒæ•°æ®ç»“æ„ |
| **å—å“ˆå¸Œè®¡ç®—** | `vllm/v1/core/kv_cache_utils.py` | `BlockHash`, `make_block_hash_with_group_id()` | Prefix Caching çš„å—æ ‡è¯† |
| **åè°ƒå™¨** | `vllm/v1/core/kv_cache_coordinator.py` | `get_kv_cache_coordinator()` | ç®¡ç†å¤šç§ KV ç¼“å­˜ç­–ç•¥ |

**é€»è¾‘å— â†’ ç‰©ç†å—æ˜ å°„çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š**

```python
# vllm/v1/core/kv_cache_utils.py:107
@dataclass
class KVCacheBlock:
    block_id: int           # ç‰©ç†å— ID (0 ~ num_gpu_blocks-1)
    ref_cnt: int = 0        # å¼•ç”¨è®¡æ•° (Prefix Caching å…±äº«)
    _block_hash: BlockHashWithGroupId | None = None  # ç”¨äºç¼“å­˜å‘½ä¸­æ£€æµ‹
```

**Preemption å†³ç­–é€»è¾‘ï¼š**

åœ¨ `scheduler.py:279-337`ï¼Œå½“ `allocate_slots()` è¿”å› `None` æ—¶ï¼š

```python
# vllm/v1/core/sched/scheduler.py:290-296
if self.policy == SchedulingPolicy.PRIORITY:
    preempted_req = max(
        self.running,
        key=lambda r: (r.priority, r.arrival_time),  # æŠ¢å ä¼˜å…ˆçº§æœ€ä½çš„
    )
else:
    preempted_req = self.running.pop()  # FCFS: æŠ¢å æœ€ååŠ å…¥çš„

self.kv_cache_manager.free(preempted_req)  # é‡Šæ”¾å…¶æ‰€æœ‰å—
preempted_req.status = RequestStatus.PREEMPTED
preempted_req.num_computed_tokens = 0  # é‡ç½®ï¼Œéœ€è¦ Recomputation
```

### è‡ªæµ‹é—®é¢˜ (Checklist)

1. **å½“ KV Cache æ»¡äº†ï¼ŒvLLM å…·ä½“åœ¨ä»£ç çš„å“ªä¸€è¡Œå†³å®šé©±é€å“ªä¸ª Sequenceï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `scheduler.py:279-337` ä¸­çš„ preemption å¾ªç¯ï¼Œç‰¹åˆ«æ˜¯ `max(self.running, key=...)` å’Œ `self.running.pop()` çš„é€‰æ‹©é€»è¾‘

2. **Prefix Caching æ˜¯å¦‚ä½•åˆ¤æ–­ä¸¤ä¸ªè¯·æ±‚å¯ä»¥å…±äº«åŒä¸€ä¸ª Block çš„ï¼Ÿ`BlockHash` æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `kv_cache_utils.py:47-67` çš„ `make_block_hash_with_group_id()` å’Œ `kv_cache_manager.py:175-215` çš„ `get_computed_blocks()`

3. **å¦‚æœä¸€ä¸ªè¢« preempted çš„è¯·æ±‚é‡æ–°è¢«è°ƒåº¦ï¼Œå®ƒçš„ `num_computed_tokens` ä¸ºä»€ä¹ˆæ˜¯ 0ï¼Ÿè¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼š`scheduler.py:326` è®¾ç½® `num_computed_tokens = 0`ï¼Œæ„å‘³ç€éœ€è¦å®Œå…¨é‡æ–°è®¡ç®—ï¼ˆRecomputationï¼‰ï¼Œå› ä¸ºå…¶ KV Cache å·²è¢«é‡Šæ”¾

---

## Phase 2: ç‰©ç†å†…å­˜ç®¡ç†ä¸ Worker æ‰§è¡Œ (The Body)

### æ ¸å¿ƒé€»è¾‘è§£æ

è¿™æ˜¯ **CPU è°ƒåº¦å†³ç­– â†’ GPU ç‰©ç†æ‰§è¡Œ** çš„æ¡¥æ¢ã€‚å…³é”®éš¾ç‚¹åœ¨äºç†è§£ `BlockTable` å¦‚ä½•ä» Python å¯¹è±¡è½¬æ¢ä¸º GPU Kernel å¯è®¿é—®çš„ Tensorã€‚

**æ•°æ®æµè½¬è¿‡ç¨‹ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SchedulerOutput (CPU)                                        â”‚
â”‚  â”œâ”€ scheduled_new_reqs: [NewRequestData, ...]                â”‚
â”‚  â”œâ”€ scheduled_cached_reqs: CachedRequestData                 â”‚
â”‚  â””â”€ num_scheduled_tokens: {req_id: num_tokens}               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPUModelRunner._prepare_inputs() (CPU â†’ GPU)                â”‚
â”‚  â”œâ”€ æ„å»º input_ids tensor                                    â”‚
â”‚  â”œâ”€ æ„å»º positions tensor                                    â”‚
â”‚  â”œâ”€ æ›´æ–° InputBatch.block_table (å…³é”®!)                      â”‚
â”‚  â””â”€ æ„å»º AttentionMetadata                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Forward (GPU)                                          â”‚
â”‚  Attention Kernel é€šè¿‡ block_tables tensor è®¿é—® KV Cache     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ä»£ç å®šä½

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | æ ¸å¿ƒç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------------|------|
| **GPU Worker** | `vllm/v1/worker/gpu_worker.py` | `Worker.execute_model()` (L543-L600+) | è°ƒç”¨ ModelRunner |
| **æ¨¡å‹è¿è¡Œå™¨** | `vllm/v1/worker/gpu_model_runner.py` | `GPUModelRunner.execute_model()` (L2650+) | æ ¸å¿ƒæ‰§è¡Œå…¥å£ |
| **è¾“å…¥æ‰¹æ¬¡ç®¡ç†** | `vllm/v1/worker/gpu_input_batch.py` | `InputBatch` | ç®¡ç† block_table tensor |
| **Attention å…ƒæ•°æ®æ„å»º** | `vllm/v1/attention/backends/utils.py` | `AttentionMetadataBuilder` | æ„å»ºä¼ ç»™ Kernel çš„å…ƒæ•°æ® |

**BlockTable ä¼ è¾“æœºåˆ¶ (å…³é”®éš¾ç‚¹)ï¼š**

åœ¨ `gpu_input_batch.py` ä¸­ï¼Œ`InputBatch` ç±»ç»´æŠ¤äº†ä¸€ä¸ª **å›ºå®šå¤§å°çš„ GPU Tensor** ç”¨äºå­˜å‚¨ block tablesï¼š

```python
# vllm/v1/worker/gpu_input_batch.py (æ¦‚å¿µç¤ºä¾‹)
class InputBatch:
    def __init__(self, ...):
        # é¢„åˆ†é… GPU tensorï¼Œé¿å…æ¯æ¬¡è°ƒåº¦éƒ½é‡æ–°åˆ†é…
        self.block_table = torch.zeros(
            (max_num_reqs, max_blocks_per_seq),
            dtype=torch.int32,
            device=device
        )

    def update_block_table(self, req_id, block_ids):
        # å°†è°ƒåº¦å™¨è¿”å›çš„ block_ids å†™å…¥å¯¹åº”ä½ç½®
        req_index = self.req_id_to_index[req_id]
        self.block_table[req_index, :len(block_ids)] = torch.tensor(block_ids)
```

**MetaData ç»„è£…æµç¨‹ï¼š**

åœ¨ `gpu_model_runner.py:2650+` çš„ `execute_model()` ä¸­ï¼š

1. è°ƒç”¨ `_update_states()` å¤„ç†æ–°è¯·æ±‚å’Œç¼“å­˜è¯·æ±‚
2. è°ƒç”¨ `_prepare_inputs()` æ„å»º `input_ids`, `positions` ç­‰
3. è°ƒç”¨ `_build_attention_metadata()` æˆ–ä½¿ç”¨ `AttentionMetadataBuilder` æ„å»º attention metadata
4. å°†æ‰€æœ‰æ•°æ®ä¼ ç»™æ¨¡å‹ `forward()`

### è‡ªæµ‹é—®é¢˜ (Checklist)

1. **åœ¨æ‰§è¡Œ `forward` ä¹‹å‰ï¼ŒMetaDataï¼ˆå¦‚ block tables, context lensï¼‰æ˜¯å¦‚ä½•è¢«ç»„è£…å¹¶ä¼ ç»™ CUDA Kernel çš„ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `gpu_model_runner.py` ä¸­çš„ `_prepare_inputs()` å’Œ `gpu_input_batch.py` ä¸­çš„ `InputBatch` ç±»çš„ `block_table` å­—æ®µæ›´æ–°é€»è¾‘

2. **`CacheEngine` æ˜¯å¦‚ä½•åˆå§‹åŒ– GPU ä¸Šçš„ KV Cache æ˜¾å­˜æ± çš„ï¼Ÿæ˜¾å­˜æ˜¯ä½•æ—¶åˆ†é…çš„ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šåœ¨ Worker åˆå§‹åŒ–é˜¶æ®µï¼Œé€šè¿‡ `initialize_kv_cache()` æ–¹æ³•åˆ†é…ï¼ŒæŸ¥çœ‹ `gpu_worker.py` ä¸­ç›¸å…³ä»£ç 

3. **ä¸ºä»€ä¹ˆ vLLM ä½¿ç”¨å›ºå®šå¤§å°çš„é¢„åˆ†é… Tensor è€Œä¸æ˜¯åŠ¨æ€åˆ†é…ï¼Ÿ**
   - ç­”æ¡ˆï¼šé¿å… CUDA å†…å­˜åˆ†é…å¼€é”€ï¼›æ”¯æŒ CUDA Graph æ•è·ï¼ˆéœ€è¦å›ºå®šå†…å­˜åœ°å€ï¼‰

---

## Phase 3: Kernel å®ç°ä¸ PagedAttention (The Heart)

### æ ¸å¿ƒé€»è¾‘è§£æ

PagedAttention çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ï¼š**å…è®¸ KV Cache åœ¨ç‰©ç†å†…å­˜ä¸­éè¿ç»­å­˜å‚¨ï¼Œé€šè¿‡ block_table é—´æ¥å¯»å€**ã€‚

è¿™ç±»ä¼¼äº OS çš„é¡µè¡¨æœºåˆ¶ï¼š
- **è™šæ‹Ÿåœ°å€** â†’ **Token åœ¨åºåˆ—ä¸­çš„ä½ç½®**
- **é¡µè¡¨** â†’ **block_table**
- **ç‰©ç†é¡µå¸§** â†’ **GPU æ˜¾å­˜ä¸­çš„ KV Block**

**Kernel å†…éƒ¨å¯»å€é€»è¾‘ï¼š**

```
Token Position (e.g., 150)
     â”‚
     â–¼
Block Index = 150 / block_size (e.g., 150/16 = 9)
     â”‚
     â–¼
Physical Block ID = block_table[seq_id][9] (e.g., 42)
     â”‚
     â–¼
KV Cache Address = kv_cache_base + 42 * block_size * head_dim
```

### å…³é”®ä»£ç å®šä½

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | æ ¸å¿ƒç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------------|------|
| **PagedAttention æ¥å£** | `vllm/attention/ops/paged_attn.py` | `PagedAttention` ç±» (L41-L263) | å°è£… CUDA ops è°ƒç”¨ |
| **Decode Attention** | `paged_attn.py:94-199` | `forward_decode()` | Decode é˜¶æ®µ (å• token) |
| **Prefill Attention** | `paged_attn.py:201-239` | `forward_prefix()` | Prefill é˜¶æ®µ (å¤š tokens) |
| **åº•å±‚ CUDA ç®—å­** | `vllm/_custom_ops.py` | `ops.paged_attention_v1/v2()` | å®é™… CUDA kernel |
| **Triton å®ç°** | `vllm/attention/ops/prefix_prefill.py` | `context_attention_fwd()` | Triton ç‰ˆæœ¬çš„ prefill |

**Kernel ç­¾ååˆ†æï¼š**

```python
# vllm/attention/ops/paged_attn.py:94-112
@staticmethod
def forward_decode(
    query: torch.Tensor,           # [num_seqs, num_heads, head_size]
    key_cache: torch.Tensor,       # [num_blocks, num_kv_heads, head_size/x, block_size, x]
    value_cache: torch.Tensor,     # [num_blocks, num_kv_heads, head_size, block_size]
    block_tables: torch.Tensor,    # [num_seqs, max_blocks_per_seq] â† å…³é”®ï¼
    seq_lens: torch.Tensor,        # [num_seqs]
    max_seq_len: int,
    kv_cache_dtype: str,
    num_kv_heads: int,
    scale: float,
    ...
) -> torch.Tensor:
```

**V1 vs V2 çš„é€‰æ‹©å¯å‘å¼ï¼š**

```python
# vllm/attention/ops/paged_attn.py:134-136
use_v1 = max_seq_len <= 8192 and (
    max_num_partitions == 1 or num_seqs * num_heads > 512
)
```

- **V1**: ä¸åˆ†åŒºï¼Œé€‚åˆçŸ­åºåˆ—æˆ–å¤§æ‰¹é‡
- **V2**: åˆ†åŒºè®¡ç®—åå½’çº¦ï¼Œé€‚åˆé•¿åºåˆ—ï¼ˆé¿å… shared memory ä¸è¶³ï¼‰

### FlashInfer é›†æˆ

vLLM ä¹Ÿæ”¯æŒ FlashInfer ä½œä¸ºé«˜æ€§èƒ½åç«¯ï¼š

```
vllm/v1/attention/backends/flashinfer.py
```

FlashInfer æä¾›äº†æ›´é«˜æ•ˆçš„ PagedAttention å®ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£ç é˜¶æ®µã€‚

### è‡ªæµ‹é—®é¢˜ (Checklist)

1. **PagedAttention Kernel ä¸­ï¼Œä¸€ä¸ª Thread Block å¤„ç†å¤šå°‘ä¸ª Tokenï¼Ÿå®ƒæ˜¯å¦‚ä½•å¤„ç† Memory Coalescingï¼ˆå†…å­˜åˆå¹¶è®¿é—®ï¼‰çš„ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ CUDA kernel å®ç°ä¸­çš„ `PARTITION_SIZE = 512` å’Œ key_cache çš„ç‰¹æ®Šå¸ƒå±€ `[num_blocks, num_kv_heads, head_size/x, block_size, x]`ï¼Œå…¶ä¸­ `x = 16 // element_size` æ˜¯ä¸ºäº†å¯¹é½å†…å­˜è®¿é—®

2. **`forward_prefix()` å’Œ `forward_decode()` æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆè¦åˆ†å¼€å®ç°ï¼Ÿ**
   - ç­”æ¡ˆï¼šPrefill å¤„ç†å¤šä¸ª query tokensï¼Œä½¿ç”¨ `context_attention_fwd`ï¼ˆç±»ä¼¼ FlashAttentionï¼‰ï¼›Decode æ¯æ¬¡åªå¤„ç†ä¸€ä¸ª tokenï¼Œä½¿ç”¨ä¸“é—¨ä¼˜åŒ–çš„ `paged_attention_v1/v2`

3. **KV Cache çš„ shape ä¸ºä»€ä¹ˆæ˜¯ `(2, num_blocks, block_size * num_kv_heads * head_size)` è€Œä¸æ˜¯ç›´æ¥å­˜å‚¨ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `paged_attn.py:47-54` çš„ `get_kv_cache_shape()` å’Œ `split_kv_cache()` æ–¹æ³•ï¼Œè¿™ç§å¸ƒå±€ä¾¿äº block swap å’Œ copy æ“ä½œ

---

## Phase 4: å¤šæ¨¡æ€æ‰©å±• (The Eyes)

### æ ¸å¿ƒé€»è¾‘è§£æ

vLLM çš„å¤šæ¨¡æ€æ¶æ„è®¾è®¡éå¸¸ä¼˜é›…ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

1. **Image/Audio Features è¢«è½¬æ¢ä¸º "Token-like" çš„ embeddings**
2. **è¿™äº› embeddings æ›¿æ¢ placeholder tokens çš„ä½ç½®**
3. **å®ƒä»¬ä¹Ÿå ç”¨ KV Cache blocks**ï¼ˆè¿™æ˜¯å¾ˆå¤šäººå¿½ç•¥çš„ï¼ï¼‰

**æ•°æ®æµï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Input: "Describe <image> this picture"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tokenizer: ["Describe", "<image>", "this", "picture"]   â”‚
â”‚             å…¶ä¸­ <image> æ˜¯ placeholder token            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MultiModalProcessor:                                     â”‚
â”‚  1. Vision Encoder: image â†’ [576 ä¸ª feature vectors]     â”‚
â”‚  2. è®¡ç®—éœ€è¦å¤šå°‘ä¸ª placeholder tokens (e.g., 576)        â”‚
â”‚  3. åœ¨ embeddings ä¸­æ›¿æ¢å¯¹åº”ä½ç½®                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Forward:                                           â”‚
â”‚  embeddings = [Describe_emb, img_feat_1, ..., img_feat_576, this_emb, picture_emb]
â”‚  â†“                                                       â”‚
â”‚  è¿™äº› embeddings å…¨éƒ¨ç»è¿‡ Attentionï¼Œäº§ç”Ÿ KV Cache       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ä»£ç å®šä½

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | æ ¸å¿ƒç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------------|------|
| **å¤šæ¨¡æ€æ³¨å†Œè¡¨** | `vllm/multimodal/registry.py` | `MultiModalRegistry` (L93-L361) | æ¨¡å‹åˆ°å¤„ç†å™¨çš„æ˜ å°„ |
| **å¤„ç†å™¨åŸºç±»** | `vllm/multimodal/processing.py` | `BaseMultiModalProcessor` | è¾“å…¥é¢„å¤„ç† |
| **è¾“å…¥ç±»å‹** | `vllm/multimodal/inputs.py` | `MultiModalKwargsItem`, `PlaceholderRange` | æ•°æ®ç»“æ„å®šä¹‰ |
| **Encoder ç¼“å­˜ç®¡ç†** | `vllm/v1/core/encoder_cache_manager.py` | `EncoderCacheManager` | ç¼“å­˜ vision encoder è¾“å‡º |
| **LLaVA å®ç°** | `vllm/model_executor/models/llava.py` | `LlavaForConditionalGeneration` | å…·ä½“æ¨¡å‹å®ç° |
| **Qwen2-VL å®ç°** | `vllm/model_executor/models/qwen2_5_vl.py` | `Qwen2_5_VLForConditionalGeneration` | æ”¯æŒ M-RoPE |

**Placeholder Token è®¡ç®—ï¼š**

åœ¨è°ƒåº¦é˜¶æ®µï¼Œéœ€è¦é¢„å…ˆè®¡ç®— image éœ€è¦å¤šå°‘ä¸ª token slotsï¼š

```python
# vllm/v1/core/sched/scheduler.py:848-857
for i, mm_feature in enumerate(mm_features):
    start_pos = mm_feature.mm_position.offset      # placeholder å¼€å§‹ä½ç½®
    num_encoder_tokens = mm_feature.mm_position.length  # éœ€è¦çš„ token æ•°é‡

    # æ£€æŸ¥è¿™ä¸ªèŒƒå›´æ˜¯å¦ä¸å½“å‰è°ƒåº¦çš„ token èŒƒå›´é‡å 
    if start_pos >= num_computed_tokens + num_new_tokens:
        break  # è¿˜æ²¡åˆ°è¿™ä¸ª encoder input
```

**EncoderCacheManager çš„ä½œç”¨ï¼š**

```python
# åœ¨ scheduler.py ä¸­
self.encoder_cache_manager = EncoderCacheManager(cache_size=encoder_cache_size)

# å½“ encoder input è¢«å¤„ç†åï¼Œç¼“å­˜å…¶è¾“å‡º
self.encoder_cache_manager.allocate(request, i)  # i æ˜¯ mm_feature çš„ç´¢å¼•

# å½“ decoder å®Œæˆè¯¥ä½ç½®çš„å¤„ç†åï¼Œé‡Šæ”¾ç¼“å­˜
self.encoder_cache_manager.free_encoder_input(request, input_id)
```

### è‡ªæµ‹é—®é¢˜ (Checklist)

1. **å¯¹äºä¸€å¼ é«˜åˆ†è¾¨ç‡å›¾ç‰‡ï¼ŒvLLM å¦‚ä½•é¢„å…ˆè®¡ç®—å®ƒéœ€è¦å ç”¨å¤šå°‘ä¸ª Token slotsï¼Œä»¥é˜²æ­¢è°ƒåº¦æ—¶æ˜¾å­˜ä¸è¶³ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `multimodal/registry.py:150-175` çš„ `get_max_tokens_per_item_by_modality()` å’Œ `multimodal/profiling.py` ä¸­çš„ `MultiModalProfiler`

2. **Image Features æ˜¯å¦å ç”¨ KV Cache Blockï¼Ÿå¦‚æœå ç”¨ï¼Œå®ƒä»¬çš„ block hash æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ**
   - ç­”æ¡ˆï¼šæ˜¯çš„ï¼Œå ç”¨ï¼å› ä¸ºå®ƒä»¬è¢«è½¬æ¢ä¸º embeddings åå‚ä¸ Attention è®¡ç®—ã€‚Hash è®¡ç®—åœ¨ `kv_cache_utils.py` ä¸­ï¼Œä¼šè€ƒè™‘ `mm_feature.identifier`ï¼ˆå¦‚ image hashï¼‰

3. **`EncoderCacheManager` å’Œ `KVCacheManager` æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªç¼“å­˜ç®¡ç†å™¨ï¼Ÿ**
   - ç­”æ¡ˆï¼š`EncoderCacheManager` ç¼“å­˜ vision encoder çš„è¾“å‡ºï¼ˆå›ºå®šå¤§å°ï¼Œä¸è¯·æ±‚å…³è”ï¼‰ï¼›`KVCacheManager` ç®¡ç† decoder çš„ KV Cacheï¼ˆåŠ¨æ€å¢é•¿ï¼‰ã€‚å‰è€…æ˜¯ä¸ºäº†é¿å…é‡å¤è®¡ç®—åŒä¸€å¼ å›¾ç‰‡çš„ encoder è¾“å‡º

---

## å®Œæ•´è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ

### LLM æ–‡æœ¬è¯·æ±‚å®Œæ•´æµç¨‹

ä¸‹é¢è¯¦ç»†è¿½è¸ªä¸€ä¸ªæ–‡æœ¬è¯·æ±‚ä»è¿›å…¥ç³»ç»Ÿåˆ°è¿”å›ç»“æœçš„å®Œæ•´è·¯å¾„ï¼š

```
ç”¨æˆ·è¯·æ±‚: "What is the capital of France?"
```

#### é˜¶æ®µ 1: API å±‚æ¥æ”¶è¯·æ±‚

**ä»£ç è·¯å¾„ï¼š**
```
vllm/entrypoints/openai/api_server.py
  â†’ vllm/entrypoints/openai/serving_chat.py
    â†’ create_chat_completion()
```

**è¾“å…¥ï¼š**
```python
{
    "model": "llama-7b",
    "messages": [{"role": "user", "content": "What is the capital of France?"}],
    "max_tokens": 100,
    "temperature": 0.7
}
```

**è¾“å‡ºï¼š**
```python
# è½¬æ¢ä¸ºå†…éƒ¨è¯·æ±‚æ ¼å¼
prompt = "What is the capital of France?"
sampling_params = SamplingParams(max_tokens=100, temperature=0.7)
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š** ç»Ÿä¸€ä¸åŒ API æ ¼å¼ï¼ˆOpenAIã€HuggingFace ç­‰ï¼‰åˆ°å†…éƒ¨è¡¨ç¤º

---

#### é˜¶æ®µ 2: Engine æ¥æ”¶å¹¶é¢„å¤„ç†

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/engine/llm_engine.py
  â†’ LLMEngine.add_request()
    â†’ vllm/v1/engine/processor.py
      â†’ Processor.process_inputs()
```

**å¤„ç†å†…å®¹ï¼š**
```python
# Tokenization
token_ids = tokenizer.encode("What is the capital of France?")
# è¾“å‡º: [1, 1724, 338, 278, 7483, 310, 3444, 29973]

# æ„å»º Request å¯¹è±¡
request = Request(
    request_id="req-001",
    prompt_token_ids=token_ids,
    sampling_params=sampling_params,
    arrival_time=time.monotonic(),
)

# è®¡ç®— block hashes (ç”¨äº prefix caching)
request.block_hashes = compute_block_hashes(token_ids, block_size=16)
```

**è¾“å‡ºï¼š**
```python
Request(
    request_id="req-001",
    prompt_token_ids=[1, 1724, 338, 278, 7483, 310, 3444, 29973],
    num_prompt_tokens=8,
    num_computed_tokens=0,
    status=RequestStatus.WAITING,
    block_hashes=[hash1]  # 8 tokens < 16, åªéœ€è¦ 1 ä¸ª block
)
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š**
- é¢„è®¡ç®— block hashes ä½¿å¾—åç»­ prefix caching æŸ¥æ‰¾ O(1)
- ç»Ÿä¸€è¯·æ±‚æ ¼å¼ä¾¿äºè°ƒåº¦å™¨å¤„ç†

---

#### é˜¶æ®µ 3: è°ƒåº¦å™¨å†³ç­–

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/core/sched/scheduler.py
  â†’ Scheduler.schedule()
    â†’ KVCacheManager.get_computed_blocks()  # æ£€æŸ¥ç¼“å­˜å‘½ä¸­
    â†’ KVCacheManager.allocate_slots()       # åˆ†é…ç‰©ç†å—
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. æ£€æŸ¥ prefix cache å‘½ä¸­
computed_blocks, num_computed_tokens = kv_cache_manager.get_computed_blocks(request)
# å‡è®¾æ²¡æœ‰å‘½ä¸­: computed_blocks=[], num_computed_tokens=0

# 2. è®¡ç®—éœ€è¦è°ƒåº¦çš„ token æ•°
num_new_tokens = request.num_tokens - num_computed_tokens  # 8 - 0 = 8

# 3. åˆ†é…ç‰©ç†å—
# 8 tokens, block_size=16, éœ€è¦ 1 ä¸ª block
new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens=8)
# è¿”å›: KVCacheBlocks(blocks=([KVCacheBlock(block_id=42)],))

# 4. æ›´æ–°è¯·æ±‚çŠ¶æ€
request.status = RequestStatus.RUNNING
request.num_computed_tokens = 8
```

**è¾“å‡ºï¼š**
```python
SchedulerOutput(
    scheduled_new_reqs=[NewRequestData(
        req_id="req-001",
        prompt_token_ids=[1, 1724, 338, 278, 7483, 310, 3444, 29973],
        block_ids=([42],),  # åˆ†é…çš„ç‰©ç†å— ID
    )],
    num_scheduled_tokens={"req-001": 8},
    total_num_scheduled_tokens=8,
)
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š**
- Prefix caching æ£€æŸ¥å¯ä»¥å¤ç”¨ä¹‹å‰è®¡ç®—çš„ KV
- æ‰¹é‡åˆ†é… blocks è€Œä¸æ˜¯é€ token åˆ†é…ï¼Œå‡å°‘ç®¡ç†å¼€é”€
- è¿”å›å®Œæ•´çš„è°ƒåº¦ç»“æœï¼Œä¾¿äº Worker æ‰§è¡Œ

---

#### é˜¶æ®µ 4: Worker å‡†å¤‡è¾“å…¥

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/worker/gpu_worker.py
  â†’ Worker.execute_model()
    â†’ vllm/v1/worker/gpu_model_runner.py
      â†’ GPUModelRunner._update_states()     # æ›´æ–°è¯·æ±‚çŠ¶æ€
      â†’ GPUModelRunner._prepare_inputs()    # å‡†å¤‡ GPU tensor
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. æ›´æ–° InputBatch çŠ¶æ€
input_batch.add_request(
    req_id="req-001",
    token_ids=[1, 1724, 338, 278, 7483, 310, 3444, 29973],
    block_ids=[42],
)

# 2. æ„å»º GPU tensors
input_ids = torch.tensor([1, 1724, 338, 278, 7483, 310, 3444, 29973], device="cuda")
positions = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], device="cuda")

# 3. æ›´æ–° block_table tensor (å…³é”®!)
# block_table[req_index] = [42, 0, 0, ...]  # åé¢è¡¥é›¶
input_batch.block_table[0, 0] = 42

# 4. æ„å»º attention metadata
attn_metadata = AttentionMetadata(
    num_prefill_tokens=8,
    num_decode_tokens=0,
    seq_lens=[8],
    block_tables=input_batch.block_table[:1],  # åªå–ç¬¬ä¸€ä¸ªè¯·æ±‚
    ...
)
```

**è¾“å‡ºï¼š**
```python
# GPU ä¸Šçš„ tensors
input_ids: torch.Tensor([1, 1724, 338, 278, 7483, 310, 3444, 29973])  # [8]
positions: torch.Tensor([0, 1, 2, 3, 4, 5, 6, 7])  # [8]
attn_metadata: AttentionMetadata(...)
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š**
- é¢„åˆ†é…å›ºå®šå¤§å° tensor æ”¯æŒ CUDA Graph
- Block table æ˜ å°„é€»è¾‘å—åˆ°ç‰©ç†å—ï¼Œå®ç°éè¿ç»­å†…å­˜è®¿é—®
- æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚æé«˜ GPU åˆ©ç”¨ç‡

---

#### é˜¶æ®µ 5: æ¨¡å‹å‰å‘ä¼ æ’­ (Prefill)

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/worker/gpu_model_runner.py
  â†’ GPUModelRunner.execute_model()
    â†’ model.forward()
      â†’ vllm/model_executor/models/llama.py
        â†’ LlamaForCausalLM.forward()
          â†’ å„å±‚çš„ Attention + FFN
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. Embedding
hidden_states = embed_tokens(input_ids)  # [8, 4096]

# 2. å„ Transformer å±‚
for layer in layers:
    # Self-Attention with PagedAttention
    # è¿™é‡Œä¼šå°† K, V å†™å…¥ block_id=42 çš„ç‰©ç†å—
    attn_output = layer.self_attn(
        hidden_states,
        positions,
        kv_cache,        # GPU ä¸Šçš„ KV ç¼“å­˜æ± 
        attn_metadata,   # åŒ…å« block_tables
    )

    # FFN
    hidden_states = layer.mlp(attn_output)

# 3. LM Head
logits = lm_head(hidden_states[-1:])  # åªå–æœ€åä¸€ä¸ªä½ç½® [1, vocab_size]
```

**KV Cache å†™å…¥ç»†èŠ‚ï¼š**
```python
# åœ¨ PagedAttention.write_to_paged_cache() ä¸­
# slot_mapping è®¡ç®—: token_position % block_size + block_id * block_size
# ä¾‹å¦‚: token 0~7 å†™å…¥ block_id=42 çš„ slot 0~7

ops.reshape_and_cache(
    key,          # [8, num_heads, head_dim]
    value,        # [8, num_heads, head_dim]
    key_cache,    # [num_blocks, ...]
    value_cache,  # [num_blocks, ...]
    slot_mapping, # [8] -> [42*16+0, 42*16+1, ..., 42*16+7]
)
```

**è¾“å‡ºï¼š**
```python
logits: torch.Tensor  # [1, 32000] æœ€åä¸€ä¸ªä½ç½®çš„ logits
# KV Cache å·²å†™å…¥ block_id=42
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š**
- Prefill é˜¶æ®µä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ prompt tokens
- KV Cache å†™å…¥ç‰©ç†å—ï¼Œåç»­ decode å¯ç›´æ¥è¯»å–
- åªè¿”å›æœ€åä¸€ä¸ªä½ç½®çš„ logits ç”¨äºé‡‡æ ·

---

#### é˜¶æ®µ 6: é‡‡æ ·

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/worker/gpu_model_runner.py
  â†’ GPUModelRunner.execute_model()
    â†’ Sampler.forward()
      â†’ vllm/v1/sample/sampler.py
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. åº”ç”¨ logits processor
logits = logits / temperature  # temperature=0.7

# 2. Top-p/Top-k é‡‡æ ·
probs = softmax(logits)
# Top-p é‡‡æ ·...

# 3. é‡‡æ · token
sampled_token = torch.multinomial(probs, num_samples=1)
# å‡è®¾é‡‡æ ·ç»“æœ: 450 (å¯¹åº” "The")
```

**è¾“å‡ºï¼š**
```python
SamplerOutput(
    sampled_token_ids=[[450]],  # "The"
)
```

---

#### é˜¶æ®µ 7: å¾ªç¯ Decode

ç°åœ¨è¿›å…¥ decode é˜¶æ®µï¼Œæ¯æ¬¡åªå¤„ç†ä¸€ä¸ª tokenï¼š

**è°ƒåº¦å™¨ï¼š**
```python
# è¯·æ±‚å·²åœ¨ RUNNING é˜Ÿåˆ—
num_new_tokens = 1  # decode æ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª token
# å¯èƒ½éœ€è¦åˆ†é…æ–°çš„ block (å½“å‰ block æ»¡äº†)
```

**Workerï¼š**
```python
# è¾“å…¥åªæœ‰ 1 ä¸ª token
input_ids = torch.tensor([450])  # "The"
positions = torch.tensor([8])     # ç¬¬ 9 ä¸ªä½ç½®

# Decode attention: query åªæœ‰ 1 ä¸ªï¼Œä½† key/value æœ‰ 9 ä¸ª
```

**å¾ªç¯ç›´åˆ°ï¼š**
- ç”Ÿæˆ EOS token
- è¾¾åˆ° max_tokens
- å…¶ä»–åœæ­¢æ¡ä»¶

---

#### é˜¶æ®µ 8: è¿”å›ç»“æœ

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/core/sched/scheduler.py
  â†’ Scheduler.update_from_output()
    â†’ æ£€æŸ¥åœæ­¢æ¡ä»¶
    â†’ æ„å»º EngineCoreOutput

vllm/v1/engine/output_processor.py
  â†’ è§£ç  token ids ä¸ºæ–‡æœ¬
```

**è¾“å‡ºï¼š**
```python
CompletionOutput(
    text="The capital of France is Paris.",
    token_ids=[450, 7483, 310, 3444, 338, 3681, 29889],
    finish_reason="stop",
)
```

---

### å¤šæ¨¡æ€è¯·æ±‚å®Œæ•´æµç¨‹

ä»¥ LLaVA æ¨¡å‹å¤„ç†å›¾æ–‡è¯·æ±‚ä¸ºä¾‹ï¼š

```
ç”¨æˆ·è¯·æ±‚:
{
    "prompt": "Describe this image: <image>",
    "images": [PIL.Image]
}
```

#### é˜¶æ®µ 1-2: ä¸æ–‡æœ¬è¯·æ±‚ç›¸åŒ

API å±‚æ¥æ”¶ï¼ŒEngine é¢„å¤„ç†

#### é˜¶æ®µ 2.5: å¤šæ¨¡æ€å¤„ç† (é¢å¤–æ­¥éª¤)

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/engine/processor.py
  â†’ Processor.process_inputs()
    â†’ vllm/multimodal/registry.py
      â†’ MultiModalRegistry.create_processor()
        â†’ vllm/multimodal/processing.py
          â†’ BaseMultiModalProcessor.apply()
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. Tokenize æ–‡æœ¬
token_ids = tokenizer.encode("Describe this image: <image>")
# [1, 4002, 29581, 445, 1967, 29901, 32000]
# å…¶ä¸­ 32000 æ˜¯ <image> placeholder token

# 2. å¤„ç†å›¾åƒ
image_processor = model.get_image_processor()
pixel_values = image_processor(image)  # [1, 3, 336, 336]

# 3. è®¡ç®—å›¾åƒéœ€è¦çš„ token æ•°é‡
# LLaVA: 336/14 * 336/14 = 576 ä¸ª patches
num_image_tokens = 576

# 4. æ„å»º MultiModal ä¿¡æ¯
mm_features = [
    MultiModalFeature(
        mm_position=PlaceholderRange(offset=6, length=576),  # <image> åœ¨ä½ç½® 6
        pixel_values=pixel_values,
        identifier=hash(image),  # ç”¨äºç¼“å­˜
    )
]

# 5. æ‰©å±• token ids (æ›¿æ¢ placeholder)
# [1, 4002, 29581, 445, 1967, 29901, 32000, 32000, ...(576ä¸ª), ...]
expanded_token_ids = expand_with_placeholders(token_ids, num_image_tokens)
```

**è¾“å‡ºï¼š**
```python
Request(
    request_id="req-002",
    prompt_token_ids=expanded_token_ids,  # é•¿åº¦ = 6 + 576 = 582
    mm_features=mm_features,
    num_computed_tokens=0,
)
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š**
- Placeholder tokens ç¡®ä¿æ–‡æœ¬å’Œå›¾åƒ token ä½ç½®æ­£ç¡®
- é¢„è®¡ç®—å›¾åƒ token æ•°é‡ï¼Œä¾¿äºè°ƒåº¦å™¨åˆ†é…å†…å­˜
- å›¾åƒ hash æ”¯æŒ encoder cache å¤ç”¨

---

#### é˜¶æ®µ 3: è°ƒåº¦å™¨å†³ç­– (è€ƒè™‘ encoder budget)

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/core/sched/scheduler.py
  â†’ Scheduler.schedule()
    â†’ _try_schedule_encoder_inputs()
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. æ£€æŸ¥ encoder budget
if encoder_compute_budget < num_image_tokens:  # 576
    # æ— æ³•è°ƒåº¦ï¼Œéœ€è¦ç­‰å¾…
    return None

# 2. åˆ†é… KV Cache blocks
# 582 tokens / 16 = 37 blocks
num_blocks = cdiv(582, 16)
new_blocks = kv_cache_manager.allocate_slots(request, num_new_tokens=582)

# 3. åˆ†é… encoder cache
encoder_cache_manager.allocate(request, mm_feature_index=0)

# 4. è®°å½•éœ€è¦è°ƒåº¦çš„ encoder inputs
scheduled_encoder_inputs["req-002"] = [0]  # ç¬¬ 0 ä¸ª mm_feature
```

**è¾“å‡ºï¼š**
```python
SchedulerOutput(
    scheduled_new_reqs=[...],
    num_scheduled_tokens={"req-002": 582},
    scheduled_encoder_inputs={"req-002": [0]},  # éœ€è¦æ‰§è¡Œ vision encoder
)
```

---

#### é˜¶æ®µ 4: Worker æ‰§è¡Œ Vision Encoder

**ä»£ç è·¯å¾„ï¼š**
```
vllm/v1/worker/gpu_model_runner.py
  â†’ GPUModelRunner.execute_model()
    â†’ _execute_mm_encoder()
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. æ‰§è¡Œ Vision Encoder
image_features = vision_tower(pixel_values)  # [1, 576, 1024]

# 2. é€šè¿‡ projector
image_features = mm_projector(image_features)  # [1, 576, 4096]

# 3. ç¼“å­˜ç»“æœ (ç”¨äºå¯èƒ½çš„å¤ç”¨)
self.encoder_cache[mm_hash] = image_features
```

---

#### é˜¶æ®µ 5: æ¨¡å‹å‰å‘ä¼ æ’­ (æ›¿æ¢ embeddings)

**ä»£ç è·¯å¾„ï¼š**
```
vllm/model_executor/models/llava.py
  â†’ LlavaForConditionalGeneration.forward()
```

**å¤„ç†å†…å®¹ï¼š**
```python
# 1. è·å–æ–‡æœ¬ embeddings
inputs_embeds = embed_tokens(input_ids)  # [582, 4096]

# 2. æ›¿æ¢ placeholder ä½ç½®çš„ embeddings
# positions 6~581 æ›¿æ¢ä¸º image_features
inputs_embeds[6:582] = image_features.squeeze(0)

# 3. æ­£å¸¸çš„ Transformer å‰å‘ä¼ æ’­
for layer in layers:
    hidden_states = layer(inputs_embeds, ...)
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤„ç†ï¼š**
- Image features è¢«è§†ä¸ºç‰¹æ®Šçš„ "tokens"
- ç»Ÿä¸€äº†æ–‡æœ¬å’Œå›¾åƒçš„å¤„ç†æµç¨‹
- KV Cache æ­£å¸¸å­˜å‚¨ï¼Œä¸åŒºåˆ†æ¥æº

---

## åˆ†å¸ƒå¼æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

vLLM æ”¯æŒå¤šç§åˆ†å¸ƒå¼ç­–ç•¥ï¼Œæ ¸å¿ƒæ˜¯ **Tensor Parallelism (TP)** å’Œ **Pipeline Parallelism (PP)**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EngineCore (å•å®ä¾‹)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚Schedulerâ”‚ â†’  â”‚ Executor  â”‚ â†’  â”‚ Workers (å¤šå®ä¾‹)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚                                   â”‚  â”‚GPU 0â”‚ â”‚GPU 1â”‚   â”‚    â”‚
â”‚                                   â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ä»£ç å®šä½

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | æ ¸å¿ƒç±» | ä½œç”¨ |
|------|---------|--------|------|
| **Executor æŠ½è±¡** | `vllm/v1/executor/abstract.py` | `Executor` | æ‰§è¡Œå™¨æ¥å£ |
| **å•è¿›ç¨‹æ‰§è¡Œå™¨** | `vllm/v1/executor/uniproc_executor.py` | `UniProcExecutor` | å• GPU |
| **å¤šè¿›ç¨‹æ‰§è¡Œå™¨** | `vllm/v1/executor/multiproc_executor.py` | `MultiprocExecutor` | å¤š GPU (TP/PP) |
| **Ray åˆ†å¸ƒå¼æ‰§è¡Œå™¨** | `vllm/v1/executor/ray_executor.py` | `RayDistributedExecutor` | å¤šèŠ‚ç‚¹ |
| **å¹¶è¡ŒçŠ¶æ€ç®¡ç†** | `vllm/distributed/parallel_state.py` | å„ç§ group å‡½æ•° | TP/PP group ç®¡ç† |
| **é€šä¿¡åŸè¯­** | `vllm/distributed/communication_op.py` | `tensor_model_parallel_all_reduce` ç­‰ | é›†åˆé€šä¿¡ |

### Tensor Parallelism (TP)

**åŸç†ï¼š** å°†æƒé‡çŸ©é˜µæŒ‰åˆ—æˆ–è¡Œåˆ‡åˆ†åˆ°å¤šä¸ª GPU

```
         GPU 0          GPU 1
        â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”
Input â†’ â”‚W[:, â”‚  â†’     â”‚W[:, â”‚ â†’ AllReduce â†’ Output
        â”‚:2048]        â”‚2048:]
        â””â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”˜
```

**ä»£ç å®ç°ï¼š**

```python
# vllm/model_executor/layers/linear.py

class ColumnParallelLinear(nn.Module):
    """
    æŒ‰åˆ—åˆ‡åˆ†æƒé‡ï¼Œæ¯ä¸ª GPU å­˜å‚¨ weight[:, start:end]
    """
    def forward(self, x):
        # æ¯ä¸ª GPU è®¡ç®—éƒ¨åˆ†è¾“å‡º
        output = F.linear(x, self.weight)  # [batch, hidden/tp]
        # ç»“æœæ‹¼æ¥ (AllGather) æˆ–ä¿æŒåˆ†ç‰‡
        return output

class RowParallelLinear(nn.Module):
    """
    æŒ‰è¡Œåˆ‡åˆ†æƒé‡ï¼Œæ¯ä¸ª GPU å­˜å‚¨ weight[start:end, :]
    """
    def forward(self, x):
        # æ¯ä¸ª GPU è®¡ç®—éƒ¨åˆ†ç»“æœ
        output = F.linear(x, self.weight)
        # AllReduce æ±‚å’Œ
        output = tensor_model_parallel_all_reduce(output)
        return output
```

**TP é€šä¿¡ä¼˜åŒ–ï¼š**

```python
# vllm/distributed/communication_op.py

def tensor_model_parallel_all_reduce(tensor):
    """
    è·¨ TP group çš„ AllReduce
    ä½¿ç”¨ NCCL å®ç°é«˜æ•ˆé€šä¿¡
    """
    if get_tensor_model_parallel_world_size() == 1:
        return tensor
    torch.distributed.all_reduce(tensor, group=get_tensor_model_parallel_group())
    return tensor
```

### Pipeline Parallelism (PP)

**åŸç†ï¼š** å°†æ¨¡å‹å±‚åˆ‡åˆ†åˆ°å¤šä¸ª GPUï¼Œå½¢æˆæµæ°´çº¿

```
GPU 0 (Layers 0-15)  â†’  GPU 1 (Layers 16-31)
     â”‚                       â”‚
     â””â”€â”€ Send hidden â”€â”€â”€â”€â”€â”€â†’ â”‚
                             â””â”€â”€ Continue forward
```

**ä»£ç å®ç°ï¼š**

```python
# vllm/v1/worker/gpu_worker.py

def execute_model(self, scheduler_output):
    # PP ç¬¬ä¸€é˜¶æ®µ
    if get_pp_group().is_first_rank:
        # ä» input_ids å¼€å§‹
        intermediate_tensors = None
    else:
        # æ¥æ”¶ä¸Šä¸€é˜¶æ®µçš„ hidden states
        intermediate_tensors = recv_from_prev_pp_rank()

    # æ‰§è¡Œæœ¬é˜¶æ®µçš„å±‚
    output = self.model_runner.execute_model(
        scheduler_output,
        intermediate_tensors,
    )

    # PP æœ€åé˜¶æ®µ
    if get_pp_group().is_last_rank:
        return output  # è¿”å›æœ€ç»ˆç»“æœ
    else:
        # å‘é€ç»™ä¸‹ä¸€é˜¶æ®µ
        send_to_next_pp_rank(output)
        return None
```

**PP è°ƒåº¦ç­–ç•¥ï¼š**

```python
# å¾®æ‰¹æ¬¡è°ƒåº¦ (1F1B)
# F: Forward, B: Backward (æ¨ç†æ—¶åªæœ‰ Forward)

# ç®€åŒ–çš„æ¨ç† pipeline:
for micro_batch in micro_batches:
    if is_first_stage:
        hidden = embed(micro_batch)
        send(hidden, next_stage)
    elif is_last_stage:
        hidden = recv(prev_stage)
        output = lm_head(forward_layers(hidden))
        outputs.append(output)
    else:
        hidden = recv(prev_stage)
        hidden = forward_layers(hidden)
        send(hidden, next_stage)
```

### Data Parallelism (DP)

**åŸç†ï¼š** æ¯ä¸ª GPU å¤„ç†ä¸åŒçš„è¯·æ±‚

```
        Request 1 â†’ GPU 0
Scheduler
        Request 2 â†’ GPU 1
```

**ä»£ç å®ç°ï¼š**

```python
# vllm/v1/worker/dp_utils.py

def coordinate_batch_across_dp(scheduler_output):
    """
    åœ¨ DP ranks ä¹‹é—´åè°ƒæ‰¹æ¬¡
    """
    # æ¯ä¸ª DP rank å¤„ç†ä¸åŒçš„è¯·æ±‚
    my_requests = scheduler_output.filter_by_dp_rank(get_dp_rank())
    return my_requests
```

### KV Cache åœ¨åˆ†å¸ƒå¼ä¸‹çš„å¤„ç†

**TP ä¸‹çš„ KV Cacheï¼š**

```python
# æ¯ä¸ª TP rank åªå­˜å‚¨ num_kv_heads / tp_size ä¸ª head çš„ KV
# ä¾‹å¦‚ tp_size=2, num_kv_heads=8
# GPU 0: heads 0-3, GPU 1: heads 4-7

kv_cache_shape = (
    2,  # K and V
    num_blocks,
    block_size * (num_kv_heads // tp_size) * head_size
)
```

**PP ä¸‹çš„ KV Cacheï¼š**

```python
# æ¯ä¸ª PP stage åªå­˜å‚¨è‡ªå·±å±‚çš„ KV Cache
# GPU 0 (layers 0-15): kv_caches[0:16]
# GPU 1 (layers 16-31): kv_caches[16:32]
```

### åˆ†å¸ƒå¼åˆå§‹åŒ–æµç¨‹

```python
# vllm/distributed/parallel_state.py

def initialize_model_parallel(
    tensor_model_parallel_size: int,
    pipeline_model_parallel_size: int,
    ...
):
    """
    åˆå§‹åŒ–å„ç§ process groups
    """
    # 1. TP group: åŒä¸€ PP stage çš„æ‰€æœ‰ ranks
    # 2. PP group: åŒä¸€ TP position çš„æ‰€æœ‰ ranks
    # 3. DP group: ç›¸åŒ TP+PP position çš„ ranks

    # åˆ›å»º NCCL groups
    for ranks in tp_groups:
        group = torch.distributed.new_group(ranks, backend="nccl")
        set_tensor_model_parallel_group(group)

    for ranks in pp_groups:
        group = torch.distributed.new_group(ranks, backend="nccl")
        set_pipeline_model_parallel_group(group)
```

### åˆ†å¸ƒå¼è‡ªæµ‹é—®é¢˜

1. **åœ¨ TP=2 çš„é…ç½®ä¸‹ï¼ŒAttention çš„ Q, K, V æŠ•å½±æ˜¯å¦‚ä½•åˆ‡åˆ†çš„ï¼ŸAllReduce å‘ç”Ÿåœ¨å“ªé‡Œï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `model_executor/layers/linear.py` çš„ `QKVParallelLinear`

2. **PP æ¨¡å¼ä¸‹ï¼Œä¸­é—´å±‚çš„ hidden states æ˜¯å¦‚ä½•åœ¨ GPU ä¹‹é—´ä¼ è¾“çš„ï¼Ÿä½¿ç”¨ä»€ä¹ˆé€šä¿¡åŸè¯­ï¼Ÿ**
   - ç­”æ¡ˆçº¿ç´¢ï¼šæŸ¥çœ‹ `distributed/communication_op.py` çš„ `send_to_next_pp_rank()` å’Œ `recv_from_prev_pp_rank()`

3. **ä¸ºä»€ä¹ˆ vLLM çš„ Scheduler æ˜¯å•ç‚¹çš„ï¼Œä¸åšåˆ†å¸ƒå¼è°ƒåº¦ï¼Ÿ**
   - ç­”æ¡ˆï¼šä¿æŒè°ƒåº¦é€»è¾‘ç®€å•ä¸€è‡´ï¼›é¿å…åˆ†å¸ƒå¼ä¸€è‡´æ€§é—®é¢˜ï¼›CPU è°ƒåº¦å¼€é”€è¿œå°äº GPU è®¡ç®—

---

## å®è·µæŒ‡å—ä¸åŠ¨æ‰‹ç»ƒä¹ 

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. Clone vLLM ä»“åº“
git clone https://github.com/vllm-project/vllm.git
cd vllm

# 2. åˆ›å»ºå¼€å‘ç¯å¢ƒ
conda create -n vllm-dev python=3.10
conda activate vllm-dev

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# 4. å®‰è£…è°ƒè¯•å·¥å…·
pip install ipdb py-spy
```

### ç»ƒä¹  1: è¿½è¸ªè¯·æ±‚ç”Ÿå‘½å‘¨æœŸ (éš¾åº¦: â­â­)

**ç›®æ ‡ï¼š** ç†è§£ä¸€ä¸ªè¯·æ±‚ä»è¿›å…¥åˆ°è¿”å›çš„å®Œæ•´è·¯å¾„

**æ­¥éª¤ï¼š**

1. **æ·»åŠ è°ƒè¯•æ‰“å°**

```python
# ä¿®æ”¹ vllm/v1/core/sched/scheduler.py
def schedule(self) -> SchedulerOutput:
    print(f"\n{'='*50}")
    print(f"[Scheduler] Starting schedule step")
    print(f"[Scheduler] Waiting queue: {len(self.waiting)}")
    print(f"[Scheduler] Running queue: {len(self.running)}")

    # ... åŸæœ‰ä»£ç  ...

    print(f"[Scheduler] Scheduled {total_num_scheduled_tokens} tokens")
    print(f"[Scheduler] New requests: {[r.req_id for r in scheduled_new_reqs]}")
    return scheduler_output
```

2. **è¿è¡Œç®€å•æµ‹è¯•**

```python
# test_trace.py
from vllm import LLM, SamplingParams

llm = LLM(model="facebook/opt-125m")  # å°æ¨¡å‹ä¾¿äºæµ‹è¯•
outputs = llm.generate(
    ["What is AI?"],
    SamplingParams(max_tokens=10)
)
print(outputs[0].outputs[0].text)
```

3. **è§‚å¯Ÿè¾“å‡º**

```bash
python test_trace.py

# é¢„æœŸçœ‹åˆ°ç±»ä¼¼è¾“å‡º:
# ==================================================
# [Scheduler] Starting schedule step
# [Scheduler] Waiting queue: 1
# [Scheduler] Running queue: 0
# [Scheduler] Scheduled 5 tokens (prefill)
# ==================================================
# [Scheduler] Starting schedule step
# [Scheduler] Waiting queue: 0
# [Scheduler] Running queue: 1
# [Scheduler] Scheduled 1 tokens (decode)
# ...
```

**æ€è€ƒé¢˜ï¼š**
- ä¸ºä»€ä¹ˆ prefill è°ƒåº¦äº† 5 ä¸ª tokens è€Œ decode åªè°ƒåº¦ 1 ä¸ªï¼Ÿ
- `waiting` å’Œ `running` é˜Ÿåˆ—çš„å˜åŒ–è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ

---

### ç»ƒä¹  2: å¯è§†åŒ– Block åˆ†é… (éš¾åº¦: â­â­â­)

**ç›®æ ‡ï¼š** ç†è§£ KV Cache Block çš„åˆ†é…å’Œé‡Šæ”¾

**æ­¥éª¤ï¼š**

1. **åˆ›å»ºå¯è§†åŒ–è„šæœ¬**

```python
# visualize_blocks.py
import matplotlib.pyplot as plt
import numpy as np
from vllm import LLM, SamplingParams

class BlockVisualizer:
    def __init__(self, num_blocks):
        self.num_blocks = num_blocks
        self.block_status = np.zeros(num_blocks)  # 0: free, 1: used
        self.history = []

    def update(self, allocated_blocks):
        self.block_status = np.zeros(self.num_blocks)
        for block_id in allocated_blocks:
            self.block_status[block_id] = 1
        self.history.append(self.block_status.copy())

    def plot(self):
        fig, ax = plt.subplots(figsize=(15, 5))
        data = np.array(self.history)
        im = ax.imshow(data.T, aspect='auto', cmap='RdYlGn_r')
        ax.set_xlabel('Schedule Step')
        ax.set_ylabel('Block ID')
        ax.set_title('KV Cache Block Allocation Over Time')
        plt.colorbar(im, label='0=Free, 1=Used')
        plt.savefig('block_allocation.png')
        print("Saved to block_allocation.png")

# ä½¿ç”¨æ–¹æ³•:
# åœ¨ scheduler.py ä¸­æ³¨å…¥ visualizer.update() è°ƒç”¨
```

2. **ä¿®æ”¹ Scheduler è®°å½•åˆ†é…**

```python
# åœ¨ scheduler.py çš„ schedule() æœ«å°¾æ·»åŠ :
all_block_ids = []
for req in self.running:
    block_ids = self.kv_cache_manager.get_block_ids(req.request_id)
    all_block_ids.extend(block_ids[0])  # å‡è®¾å• group

# è°ƒç”¨å¯è§†åŒ–æ›´æ–°
if hasattr(self, 'visualizer'):
    self.visualizer.update(all_block_ids)
```

3. **è¿è¡Œå¤šè¯·æ±‚æµ‹è¯•**

```python
llm = LLM(model="facebook/opt-125m", gpu_memory_utilization=0.5)

# å¹¶å‘å¤šä¸ªè¯·æ±‚
prompts = [
    "Tell me a story about",
    "Explain quantum physics",
    "Write a poem about",
]
outputs = llm.generate(prompts, SamplingParams(max_tokens=50))
```

**æ€è€ƒé¢˜ï¼š**
- ä¸ºä»€ä¹ˆä¸åŒè¯·æ±‚çš„ blocks ä¸è¿ç»­ï¼Ÿ
- å½“ä¸€ä¸ªè¯·æ±‚å®Œæˆåï¼Œå®ƒçš„ blocks è¢«å¤ç”¨äº†å—ï¼Ÿ

---

### ç»ƒä¹  3: å®ç°ç®€å•çš„è°ƒåº¦ç­–ç•¥ (éš¾åº¦: â­â­â­â­)

**ç›®æ ‡ï¼š** æ·±å…¥ç†è§£è°ƒåº¦å™¨å¦‚ä½•åšå†³ç­–

**ä»»åŠ¡ï¼š** å®ç°ä¸€ä¸ª "Shortest Job First" è°ƒåº¦ç­–ç•¥

```python
# vllm/v1/core/sched/request_queue.py

class SJFRequestQueue(RequestQueue):
    """
    Shortest Job First: ä¼˜å…ˆè°ƒåº¦å‰©ä½™ token æœ€å°‘çš„è¯·æ±‚
    """

    def add_request(self, request: Request):
        # æŒ‰ (remaining_tokens, arrival_time) æ’åº
        remaining = request.max_tokens - len(request.output_token_ids)
        heapq.heappush(self._queue, (remaining, request.arrival_time, request))

    def peek_request(self) -> Request:
        return self._queue[0][2]

    def pop_request(self) -> Request:
        return heapq.heappop(self._queue)[2]
```

**æµ‹è¯•ï¼š**

```python
# æ¯”è¾ƒ FCFS å’Œ SJF çš„å¹³å‡å»¶è¿Ÿ
requests = [
    ("Short prompt", SamplingParams(max_tokens=10)),
    ("Long prompt " * 100, SamplingParams(max_tokens=100)),
    ("Medium prompt " * 10, SamplingParams(max_tokens=50)),
]

# æµ‹é‡ä¸åŒç­–ç•¥ä¸‹çš„å¹³å‡å®Œæˆæ—¶é—´
```

---

### ç»ƒä¹  4: åˆ†æ PagedAttention Kernel (éš¾åº¦: â­â­â­â­â­)

**ç›®æ ‡ï¼š** æ·±å…¥ CUDA kernel å®ç°

**æ­¥éª¤ï¼š**

1. **ä½¿ç”¨ NSight Systems åˆ†æ**

```bash
nsys profile -o vllm_profile python test_inference.py
nsys-ui vllm_profile.nsys-rep
```

2. **å®šä½ PagedAttention kernel**

åœ¨ NSight ä¸­æœç´¢:
- `paged_attention_v1_kernel`
- `paged_attention_v2_kernel`

3. **åˆ†æ kernel å‚æ•°**

```python
# åœ¨ vllm/attention/ops/paged_attn.py çš„ forward_decode() å‰æ·»åŠ :
print(f"Query shape: {query.shape}")
print(f"Block tables shape: {block_tables.shape}")
print(f"Max seq len: {max_seq_len}")
print(f"Using V1: {use_v1}")
```

4. **å®éªŒä¸åŒé…ç½®**

```python
# æµ‹è¯•ä¸åŒ batch size å’Œ seq len å¯¹ kernel é€‰æ‹©çš„å½±å“
for batch_size in [1, 8, 64, 512]:
    for seq_len in [128, 1024, 8192, 16384]:
        # è®°å½•ä½¿ç”¨ V1 è¿˜æ˜¯ V2
        # è®°å½•æ‰§è¡Œæ—¶é—´
```

**æ€è€ƒé¢˜ï¼š**
- V1 å’Œ V2 çš„é€‰æ‹©å¯å‘å¼æ˜¯å¦æœ€ä¼˜ï¼Ÿèƒ½å¦æ”¹è¿›ï¼Ÿ
- ä¸åŒ block size å¯¹æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

---

### ç»ƒä¹  5: å¤šæ¨¡æ€è°ƒè¯• (éš¾åº¦: â­â­â­)

**ç›®æ ‡ï¼š** ç†è§£å›¾åƒå¦‚ä½•å˜æˆ "tokens"

**æ­¥éª¤ï¼š**

1. **è¿½è¸ª placeholder å±•å¼€**

```python
# ä¿®æ”¹ vllm/multimodal/processing.py
class BaseMultiModalProcessor:
    def apply(self, ...):
        # æ‰“å°åŸå§‹ token æ•°é‡
        print(f"Original tokens: {len(token_ids)}")

        result = self._apply(...)

        # æ‰“å°å±•å¼€åæ•°é‡
        print(f"After expansion: {len(result.prompt_token_ids)}")
        print(f"Image tokens added: {len(result.prompt_token_ids) - len(token_ids)}")

        return result
```

2. **å¯è§†åŒ– image features**

```python
# test_multimodal.py
from vllm import LLM, SamplingParams
from PIL import Image

llm = LLM(
    model="llava-hf/llava-1.5-7b-hf",
    max_model_len=4096,
)

image = Image.open("test.jpg")
prompt = "<image>\nDescribe this image."

outputs = llm.generate(
    [{
        "prompt": prompt,
        "multi_modal_data": {"image": image}
    }],
    SamplingParams(max_tokens=100)
)
```

3. **æ£€æŸ¥ encoder cache å‘½ä¸­**

```python
# ç›¸åŒå›¾ç‰‡å‘é€ä¸¤æ¬¡ï¼Œæ£€æŸ¥ encoder cache æ˜¯å¦å¤ç”¨
for i in range(2):
    outputs = llm.generate([{...}])
    # æ£€æŸ¥æ—¥å¿—ä¸­çš„ cache hit
```

---

### ç»ƒä¹  6: åˆ†å¸ƒå¼è°ƒè¯• (éš¾åº¦: â­â­â­â­â­)

**ç›®æ ‡ï¼š** ç†è§£ TP/PP ä¸‹çš„é€šä¿¡

**å‰æï¼š** éœ€è¦å¤š GPU ç¯å¢ƒ

**æ­¥éª¤ï¼š**

1. **å¯åŠ¨ TP=2 é…ç½®**

```python
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    tensor_parallel_size=2,
)
```

2. **è¿½è¸ª AllReduce æ“ä½œ**

```python
# ä¿®æ”¹ vllm/distributed/communication_op.py
def tensor_model_parallel_all_reduce(tensor):
    if get_tensor_model_parallel_world_size() == 1:
        return tensor

    # æ·»åŠ è°ƒè¯•
    rank = get_tensor_model_parallel_rank()
    print(f"[TP Rank {rank}] AllReduce tensor shape: {tensor.shape}")

    torch.distributed.all_reduce(tensor, group=get_tensor_model_parallel_group())
    return tensor
```

3. **æµ‹é‡é€šä¿¡å¼€é”€**

```python
import torch.cuda.nvtx as nvtx

def tensor_model_parallel_all_reduce(tensor):
    with nvtx.annotate("AllReduce", color="red"):
        torch.distributed.all_reduce(...)
```

ç„¶åç”¨ NSight æŸ¥çœ‹ AllReduce åœ¨æ€»æ—¶é—´ä¸­çš„å æ¯”ã€‚

---

### æ¨èå­¦ä¹ è·¯å¾„

#### ç¬¬ä¸€å‘¨ï¼šç†Ÿæ‚‰å…¥å£
1. è¿è¡Œå®˜æ–¹ examples
2. å®Œæˆç»ƒä¹  1
3. é˜…è¯» `scheduler.py` å‰ 200 è¡Œ

#### ç¬¬äºŒå‘¨ï¼šæ·±å…¥è°ƒåº¦
1. å®Œæˆç»ƒä¹  2
2. é˜…è¯» `kv_cache_manager.py` å…¨éƒ¨
3. å°è¯•ç»ƒä¹  3

#### ç¬¬ä¸‰å‘¨ï¼šç†è§£æ‰§è¡Œ
1. é˜…è¯» `gpu_model_runner.py` çš„ `execute_model()`
2. è¿è¡Œ NSight åˆ†æ
3. å¼€å§‹ç»ƒä¹  4

#### ç¬¬å››å‘¨ï¼šä¸“é¡¹æ·±å…¥
1. å¦‚æœå¯¹å¤šæ¨¡æ€æ„Ÿå…´è¶£ â†’ ç»ƒä¹  5
2. å¦‚æœå¯¹åˆ†å¸ƒå¼æ„Ÿå…´è¶£ â†’ ç»ƒä¹  6
3. å¦‚æœå¯¹æ€§èƒ½ä¼˜åŒ–æ„Ÿå…´è¶£ â†’ æ·±å…¥ CUDA kernel

### è°ƒè¯•æŠ€å·§

1. **æ—¥å¿—çº§åˆ«**
```bash
export VLLM_LOGGING_LEVEL=DEBUG
```

2. **æ‰“æ–­ç‚¹**
```python
import ipdb; ipdb.set_trace()
```

3. **GPU Profiling**
```bash
nsys profile -o output python script.py
```

4. **å†…å­˜åˆ†æ**
```python
torch.cuda.memory_summary()
```

5. **åˆ†å¸ƒå¼è°ƒè¯•**
```bash
# åªåœ¨ rank 0 æ‰“å°
if torch.distributed.get_rank() == 0:
    print(...)
```

### è´¡çŒ®ä»£ç å»ºè®®

1. **ä»å° issue å¼€å§‹**
   - æ–‡æ¡£æ”¹è¿›
   - ç±»å‹æ³¨è§£è¡¥å……
   - å° bug ä¿®å¤

2. **é˜…è¯»å·²æœ‰ PR**
   - å­¦ä¹ ä»£ç é£æ ¼
   - ç†è§£ review æµç¨‹

3. **å‚ä¸è®¨è®º**
   - GitHub Issues
   - Discord ç¤¾åŒº

4. **æ€§èƒ½ä¼˜åŒ–æ–¹å‘**
   - æ–°çš„è°ƒåº¦ç­–ç•¥
   - Kernel ä¼˜åŒ–
   - å†…å­˜ç®¡ç†æ”¹è¿›

---

## ä»£ç é˜…è¯»è·¯å¾„æ€»ç»“

### å…¥é—¨è·¯å¾„ï¼ˆ2-3å¤©ï¼‰
```
scheduler.py â†’ kv_cache_manager.py â†’ gpu_model_runner.py (å‰500è¡Œ)
```

### æ·±å…¥è·¯å¾„ï¼ˆ1å‘¨ï¼‰
```
scheduler.py (å…¨éƒ¨)
  â†’ kv_cache_utils.py (KVCacheBlock, FreeKVCacheBlockQueue)
  â†’ gpu_model_runner.py (execute_model, _prepare_inputs)
  â†’ paged_attn.py (forward_decode, forward_prefix)
```

### å¤šæ¨¡æ€ä¸“é¡¹ï¼ˆ3å¤©ï¼‰
```
multimodal/registry.py â†’ multimodal/processing.py
  â†’ encoder_cache_manager.py
  â†’ model_executor/models/llava.py æˆ– qwen2_5_vl.py
```

### åˆ†å¸ƒå¼ä¸“é¡¹ï¼ˆ3å¤©ï¼‰
```
distributed/parallel_state.py â†’ executor/multiproc_executor.py
  â†’ worker/gpu_worker.py
  â†’ model_executor/layers/linear.py (Parallel Linear)
```

---

## å‚è€ƒèµ„æº

1. **å®˜æ–¹æ–‡æ¡£**: https://docs.vllm.ai/
2. **è®ºæ–‡**: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
3. **æºç **: https://github.com/vllm-project/vllm
4. **Discord**: https://discord.gg/vllm

---

> æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ï¼Œæœ€åæ›´æ–°æ—¶é—´: 2025-11-20
